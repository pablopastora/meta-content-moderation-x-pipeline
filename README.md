# Media reactions on X to Meta’s content moderation policy  
## Data collection and analysis pipeline

This repository contains the **computational pipeline** used in the article:

> *Media reactions on X to the change in Meta’s content moderation policy:  
> A comparative study of Spain, the United States and Turkey*

The pipeline combines **automated data collection**, **descriptive and lexical analysis of media posts**, and **user-reply extraction**, implemented primarily in **R**, together with a **sentiment analysis stage implemented in Python**.

Due to ethical and copyright restrictions surrounding social media data, **no raw social media content (texts or replies) is shared in this repository**. The scripts provided here allow other researchers to replicate the data collection and analysis procedures in accordance with platform policies.


## Overview of the pipeline

The workflow is organised into five sequential scripts:

1. **`01_scrape_tweets_meta.R`**  
   Scrapes posts published by selected news media outlets on X (formerly Twitter) using advanced search queries related to Meta’s content moderation policy.

2. **`02_build_urls_meta.R`**  
   Cleans and structures the scraped dataset, generating a table of Tweet IDs, URLs and associated metadata (media outlet, country, publication date).

3. **`03_extract_replies_meta.R`**  
   Extracts all user replies associated with the selected media posts using the `twscrapeR` package.

4. **`04_descriptive_lexical_media_posts.R`**  
   Performs a preliminary descriptive analysis of media posts, including posting frequency and engagement metrics (replies, reposts and likes), as well as lexical frequency analysis, word clouds and bigram extraction by country.

5. **`05_sentiments_meta.ipynb`**  
   Translates user replies into English and performs sentiment analysis using transformer-based language models. This notebook implements the polarity modelling reported in the article and is provided for transparency and methodological completeness.


## Data availability

This repository includes the file:

- **`content_moderation_meta_ids.csv`**

This dataset contains **Tweet IDs and User IDs only**, without any tweet texts or reply contents.  
It is provided to allow other researchers to **rehydrate the data** using X’s API, in accordance with the platform’s terms of service.

The underlying data to this research **cannot be shared in full** due to ethical and copyright restrictions surrounding social media data. The Methods section of the article and the scripts in this repository provide sufficient detail to allow replication of the study.

**Disclaimer required by the journal**:  
> *The underlying data to this research cannot be shared due to the ethical and copyright restrictions surrounding social media data. The Methods section contains detailed information to allow replication of the study. Any queries about the methodology should be directed to the corresponding author.*


## Directory structure

- `scripts/` — R scripts and Python notebook implementing the full pipeline  
- `data/` — Contains `content_moderation_meta_ids.csv` (Tweet and User IDs only)

No raw social media texts are stored in this repository.


## Software requirements

### R (version ≥ 4.3)

Required packages:
- `TweetScraperR`
- `twscrapeR`
- `dplyr`
- `readr`
- `stringr`
- `lubridate`
- `ggplot2`
- `tidytext`
- `stopwords`
- `tibble`

### Python (version ≥ 3.9)

Required libraries:
- `pandas`
- `numpy`
- `torch`
- `transformers`
- `sentencepiece`
- `matplotlib`

All software dependencies are open source.


## Authentication and credentials

No credentials are hardcoded in any script.

Authentication details for accessing X must be provided via **environment variables**.


## TweetScraperR (media posts)

```r
Sys.setenv(
  USER = "your_x_username",
  PASS = "your_x_password"
)
```
## TwscrapeR (user replies)
```r
Sys.setenv(
  TW_USER = "your_x_username",
  TW_PASS = "your_x_password",
  TW_EMAIL = "your_email",
  TW_EMAIL_PASS = "your_email_password",
  TW_COOKIES = "auth_token=...; ct0=..."
)
```

## Execution order

Scripts should be executed in the following order:

01_scrape_tweets_meta.R

02_build_urls_meta.R

03_extract_replies_meta.R

04_descriptive_lexical_media_posts.R

05_sentiments_meta.ipynb

Each script depends on the outputs generated by the previous steps.


## Outputs

The pipeline generates:

Descriptive statistics on posting frequency and engagement metrics

Lexical frequency tables, word clouds and bigram rankings by country

A dataset of user replies linked to media posts (not shared)

Polarity scores derived from sentiment analysis of translated user replies

No raw social media texts are distributed.


## Ethical considerations

In accordance with X’s terms of service and Horizon Europe data policies:

No tweet texts or replies are reproduced

No screenshots or representative posts are included

Only Tweet IDs and User IDs are shared for rehydration purposes

This repository follows the principle “as open as possible, as closed as necessary.”


## License

Code: Creative Commons Attribution 4.0 International (CC BY 4.0)

Dataset (content_moderation_meta_ids.csv): CC BY 4.0

Reuse is permitted provided appropriate credit is given to the author.


## Citation

If you use this code or dataset, please cite it as:

Pastora Estébanez, P. (2026). Data collection and analysis pipeline for media reactions on X to Meta’s content moderation policy. GitHub repository and Zenodo archive. DOI.
